{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CUknot/NLP/blob/main/Lab2_1_language_modeling_to_student_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "15QfB7RAuXAc"
      },
      "source": [
        "# Language Modeling using Ngram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gucid6KNuXAe"
      },
      "source": [
        "In this Exercise, we are going to create a bigram language model and its variation. We will build one model for each of the following type and calculate their perplexity:\n",
        "- Unigram Model\n",
        "- Bigram Model\n",
        "- Bigram Model with Laplace smoothing\n",
        "- Bigram Model with Interpolation\n",
        "- Bigram Model with Kneser-ney Interpolation\n",
        "\n",
        "We will also use NLTK which is a natural language processing library for python to make our lives easier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRRrn78ZjL54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d27eb34-3c95-4656-dd0b-03ee104b1687"
      },
      "source": [
        "# #download corpus\n",
        "!wget --no-check-certificate https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n",
        "!unzip BEST2010.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-17 15:46:59--  https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip [following]\n",
            "--2025-01-17 15:46:59--  https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7423530 (7.1M) [application/zip]\n",
            "Saving to: ‘BEST2010.zip’\n",
            "\n",
            "BEST2010.zip        100%[===================>]   7.08M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-01-17 15:46:59 (176 MB/s) - ‘BEST2010.zip’ saved [7423530/7423530]\n",
            "\n",
            "Archive:  BEST2010.zip\n",
            "   creating: BEST2010/\n",
            "  inflating: BEST2010/article.txt    \n",
            "  inflating: BEST2010/encyclopedia.txt  \n",
            "  inflating: BEST2010/news.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/jajdlqnp5h0ywvo/tokenized_wiki_sample.csv"
      ],
      "metadata": {
        "id": "qeyvLSptdKXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dbea5d6-7fb4-4ed9-fd10-33d71d955d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-17 15:47:00--  https://www.dropbox.com/s/jajdlqnp5h0ywvo/tokenized_wiki_sample.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/88uzig0mno1b57d6bhwht/tokenized_wiki_sample.csv?rlkey=oya9jw1rljj31jc49fvoaty01 [following]\n",
            "--2025-01-17 15:47:00--  https://www.dropbox.com/scl/fi/88uzig0mno1b57d6bhwht/tokenized_wiki_sample.csv?rlkey=oya9jw1rljj31jc49fvoaty01\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc46f2d4108cb803453139054555.dl.dropboxusercontent.com/cd/0/inline/CiVREFBCieee2d_FVpqjhQD85FoSXcs7Nds65HkGoo3B5BTrpUqviaI2_2zdjA5CENRBhk5o9Uk9ARCS_BgiPxFy447e0pK8iYqzGqL7moXeZHgdOVgJpqYOmKh4kuF1lIm6ZewXEJQY5sjV5FpTcnec/file# [following]\n",
            "--2025-01-17 15:47:00--  https://uc46f2d4108cb803453139054555.dl.dropboxusercontent.com/cd/0/inline/CiVREFBCieee2d_FVpqjhQD85FoSXcs7Nds65HkGoo3B5BTrpUqviaI2_2zdjA5CENRBhk5o9Uk9ARCS_BgiPxFy447e0pK8iYqzGqL7moXeZHgdOVgJpqYOmKh4kuF1lIm6ZewXEJQY5sjV5FpTcnec/file\n",
            "Resolving uc46f2d4108cb803453139054555.dl.dropboxusercontent.com (uc46f2d4108cb803453139054555.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uc46f2d4108cb803453139054555.dl.dropboxusercontent.com (uc46f2d4108cb803453139054555.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4399477 (4.2M) [text/plain]\n",
            "Saving to: ‘tokenized_wiki_sample.csv’\n",
            "\n",
            "tokenized_wiki_samp 100%[===================>]   4.20M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-01-17 15:47:01 (63.0 MB/s) - ‘tokenized_wiki_sample.csv’ saved [4399477/4399477]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjJDeG03uXAf"
      },
      "source": [
        "#First we import necessary library such as math, nltk, bigram, and collections.\n",
        "import math\n",
        "import nltk\n",
        "import io\n",
        "import random\n",
        "from random import shuffle\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "random.seed(999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HugXBHNEuXAh"
      },
      "source": [
        "BEST2010 is a free Thai NLP dataset by NECTEC usually used as a standard benchmark for various NLP tasks including language modeling. It is separated into 4 domains including article, encyclopedia, news, and novel. The data is already  tokenized using '|' as a separator.\n",
        "\n",
        "For example,\n",
        "\n",
        "ตาม|ที่|นางประนอม ทองจันทร์| |กับ| |ด.ช.กิตติพงษ์ แหลมผักแว่น| |และ| |ด.ญ.กาญจนา กรองแก้ว| |ป่วย|สงสัย|ติด|เชื้อ|ไข้|ขณะ|นี้|ยัง|ไม่|ดี|ขึ้น|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu-AJSZZuXAi"
      },
      "source": [
        "total_word_count = 0\n",
        "best2010 = []\n",
        "with open('BEST2010/news.txt','r',encoding='utf-8') as f:\n",
        "  for i,line in enumerate(f):\n",
        "    line=line.strip()[:-1] #remove the trailing |\n",
        "    total_word_count += len(line.split(\"|\"))\n",
        "    best2010.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WfpGgbruXAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97811a18-346e-423b-a270-ce4bbcc6cd0c"
      },
      "source": [
        "#For simplicity, we assumes that each line is a sentence.\n",
        "print (f'Total sentences in BEST2010 news dataset :\\t{len(best2010)}')\n",
        "print (f'Total word counts in BEST2010 news dataset :\\t{total_word_count}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences in BEST2010 news dataset :\t30969\n",
            "Total word counts in BEST2010 news dataset :\t1660190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JD9iXF1uXAm"
      },
      "source": [
        "We separate the input into 2 sets, train and test data with 70:30 ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WGcQq_juXAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984aed0a-3cf9-48c6-b2d6-e40b30e8053d"
      },
      "source": [
        "sentences = best2010\n",
        "# The data is separated to train and test set with 70:30 ratio.\n",
        "train = sentences[:int(len(sentences)*0.7)]\n",
        "test = sentences[int(len(sentences)*0.7):]\n",
        "\n",
        "#Training data\n",
        "train_word_count =0\n",
        "for line in train:\n",
        "    for word in line.split('|'):\n",
        "        train_word_count+=1\n",
        "print ('Total sentences in BEST2010 news training dataset :\\t'+ str(len(train)))\n",
        "print ('Total word counts in BEST2010 news training dataset :\\t'+ str(train_word_count))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences in BEST2010 news training dataset :\t21678\n",
            "Total word counts in BEST2010 news training dataset :\t1042797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load the data from Wikipedia which is also already tokenized. It will be used for answering questions in MyCourseville."
      ],
      "metadata": {
        "id": "17x6tW-3ae7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "wiki_data = pd.read_csv(\"tokenized_wiki_sample.csv\")"
      ],
      "metadata": {
        "id": "0fAl6dTg_9HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Before training any language models, the first step we always do is process the data into the format suited for the LM.\n",
        "\n",
        "For this exercise, we will use NLTK to help process our data."
      ],
      "metadata": {
        "id": "H1W5bm-hbQXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import pad_both_ends, flatten\n",
        "from nltk.lm.vocabulary import Vocabulary\n",
        "from nltk import ngrams"
      ],
      "metadata": {
        "id": "4OIqxJB7P29D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by \"tokenizing\" our training set. Note that the data is already tokenized so we can just split it."
      ],
      "metadata": {
        "id": "Oy0ZN2_0bzRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = [[\"<s>\"] + t.split(\"|\") + [\"</s>\"] for t in train] # \"tokenize\" and pad each sentence"
      ],
      "metadata": {
        "id": "WQM0PXnXbzCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we create a vocabulary with the ```Vocabulary``` class from NLTK. It accepts a list of tokens so we flatten our sentences into one long sentence first.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TM2ylNRNcrg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flat_tokens = list(flatten(tokenized_train)) #join all sentences into one long sentence\n",
        "vocab = Vocabulary(flat_tokens, unk_cutoff=3) #Words with frequency **below** 3 (not exactly 3) will not be considered in our vocab and will be converted to <UNK>."
      ],
      "metadata": {
        "id": "Tbp-VmkHcq4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we replace low frequency words.\n",
        "\n",
        "Now *each* sentence is going to look something like this:\n",
        "\\[\"\\<s\\>\", \"hello\", \"my\", \"name\", \"is\", \"\\<UNK\\>\", \"\\</s\\>\" \\]"
      ],
      "metadata": {
        "id": "oFnBHe6ScAaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = [[token if token in vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_train]"
      ],
      "metadata": {
        "id": "9q6QakuibxqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we do the same for the test set and the wiki dataset."
      ],
      "metadata": {
        "id": "Dn6GxaSFeSpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test = [t.split(\"|\") for t in test]\n",
        "tokenized_test = [[token if token in vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_test]\n",
        "\n",
        "tokenized_wiki_test = [t.split(\"|\") for t in wiki_data['tokenized'].tolist()]\n",
        "tokenized_wiki_test = [[token if token in vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_wiki_test]"
      ],
      "metadata": {
        "id": "D4N6qKrPadIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHtCMFMluXAo"
      },
      "source": [
        "# Unigram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V1WQTGzuXAp"
      },
      "source": [
        "In this section, we will demonstrate how to build a unigram language model <br>\n",
        "**Important note:** <br>\n",
        "**\\<s\\>** = sentence start symbol <br>\n",
        "**\\</s\\>** = sentence end symbol"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VERY IMPORTANT:\n",
        "- In this notebook, we will *not* default the unknown token probability to ```1/len(vocab)``` but instead will treat it as a normal word and let the model learn its probability so that we can compare our results to NLTK.\n",
        "- **Also make sure that the code in this notebook can be executed without any problem. If we find that you used NLTK to answer questions in MyCourseVille and did not finish the assignment, you will receive a grade of 0 for this assignment.**"
      ],
      "metadata": {
        "id": "Xd7qOd7KAYWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnigramModel():\n",
        "  def __init__(self, data, vocab):\n",
        "    self.unigram_count = defaultdict(lambda: 0.0)\n",
        "    self.word_count = 0\n",
        "    self.vocab = vocab\n",
        "    for sentence in data:\n",
        "        for w in sentence: #[(word1, ), (word2, ), (word3, )...]\n",
        "          w = w[0]\n",
        "          self.unigram_count[w] +=1.0\n",
        "          self.word_count+=1\n",
        "\n",
        "  def __getitem__(self, w):\n",
        "    w = w[0]  #[(word1, ), (word2, ), (word3, )...]\n",
        "    if w in self.vocab:\n",
        "      return self.unigram_count[w]/(self.word_count)\n",
        "    else:\n",
        "      return self.unigram_count[\"<UNK>\"]/(self.word_count)"
      ],
      "metadata": {
        "id": "CTV-i9kdse58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_unigrams = [list(ngrams(sent, n=1)) for sent in tokenized_train] #creating the unigrams by setting n=1\n",
        "model = UnigramModel(train_unigrams, vocab)"
      ],
      "metadata": {
        "id": "FnWJJ8Hqs8Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6coGxSY7uXAt"
      },
      "source": [
        "def getLnValue(x):\n",
        "      if x == 0:\n",
        "        return -math.inf\n",
        "      return math.log(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFy8yhZjuXAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c46adc5-235f-445a-868d-cbe72541c891"
      },
      "source": [
        "#problability of 'นายก'\n",
        "print(getLnValue(model[('นายก',)]))\n",
        "\n",
        "#for example, problability of 'นายกรัฐมนตรี' which is an unknown word is equal to\n",
        "print(getLnValue(model[('นายกรัฐมนตรี',)]))\n",
        "\n",
        "#problability of 'นายก' 'ได้' 'ให้' 'สัมภาษณ์' 'กับ' 'สื่อ'\n",
        "prob = getLnValue(model[('นายก',)])+getLnValue(model[('ได้',)])+ getLnValue(model[('ให้',)])+getLnValue(model[('สัมภาษณ์',)])+getLnValue(model[('กับ',)])+getLnValue(model[('สื่อ',)])+getLnValue(model[('</s>',)])\n",
        "print ('Problability of a sentence', math.exp(prob))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-6.571687039690381\n",
            "-3.952132570275872\n",
            "Problability of a sentence 4.877889285183675e-18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8EfqnDsuXAw"
      },
      "source": [
        "# Perplexity\n",
        "\n",
        "In order to compare language model we need to calculate perplexity. In this task you should write a perplexity calculation code for the unigram model. The result perplexity should be around 448.90 and\n",
        "392.74 on train and test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZHQ-6tVuXAx"
      },
      "source": [
        "## TODO #1 Calculate perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh0DwzoouXAx"
      },
      "source": [
        "def getLnValue(x):\n",
        "    if x == 0:\n",
        "        return -math.inf\n",
        "    return math.log(x)\n",
        "\n",
        "def calculate_sentence_ln_prob(sentence, model):\n",
        "    ln_prob = 0\n",
        "    for word in sentence:\n",
        "        prob = model[word]\n",
        "        ln_prob += getLnValue(prob)\n",
        "    return ln_prob\n",
        "\n",
        "def perplexity(test,model):\n",
        "    total_ln_prob = 0\n",
        "    total_words = 0\n",
        "\n",
        "    for sentence in test:\n",
        "        total_ln_prob += calculate_sentence_ln_prob(sentence, model)\n",
        "        total_words += len(sentence)\n",
        "\n",
        "    avg_ln_prob = total_ln_prob / total_words\n",
        "    return math.exp(-avg_ln_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_unigrams = [list(ngrams(sent, n=1)) for sent in tokenized_test]"
      ],
      "metadata": {
        "id": "X-t_8mEzRxT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(perplexity(train_unigrams,model))\n",
        "print(perplexity(test_unigrams,model))"
      ],
      "metadata": {
        "id": "PztVYprdtBja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ef5930-16bb-49ca-8bef-4442679a7dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "448.89690751824827\n",
            "392.74028966757214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1 MCV\n",
        "Calculate the perplexity of the model on the wiki test set and answer in MyCourseVille"
      ],
      "metadata": {
        "id": "PHnBXtt3b-OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_test_unigrams = [list(ngrams(sent, n=1)) for sent in tokenized_wiki_test]"
      ],
      "metadata": {
        "id": "JRd6hF_WSBl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(perplexity([list(flatten(wiki_test_unigrams))], model))"
      ],
      "metadata": {
        "id": "I_LiSohADNLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ae2102-6b57-400a-a789-3483aad12ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "485.7336366066887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK0gaMf0uXA2"
      },
      "source": [
        "# Bigram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmTkAY_QuXA3"
      },
      "source": [
        "Next, you will create a better language model than a unigram (which is not much to compare with). But first, it is very tedious to count every pair of words that occur in our corpus by ourselves. Lucky for us, nltk provides us a simple library which will simplify the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv6r2LJ1uXA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05e9584-d57a-461e-ea2c-e104e4ffd29d"
      },
      "source": [
        "#example of nltk usage for bigram\n",
        "sentence = 'I always search google for an answer .'\n",
        "padded_sentence = list(pad_both_ends(sentence.split(), n=2))\n",
        "\n",
        "print('This is how nltk generate bigram.')\n",
        "for w1,w2 in bigrams(padded_sentence):\n",
        "    print(w1,w2)\n",
        "print('\\n<s> and </s> are used as a start and end of sentence symbol. respectively.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is how nltk generate bigram.\n",
            "<s> I\n",
            "I always\n",
            "always search\n",
            "search google\n",
            "google for\n",
            "for an\n",
            "an answer\n",
            "answer .\n",
            ". </s>\n",
            "\n",
            "<s> and </s> are used as a start and end of sentence symbol. respectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R2T-6i9uXA6"
      },
      "source": [
        "Now, you should be able to implement a bigram model by yourself. Also, you must create a new perplexity calculation for bigram. The result perplexity should be around 56.46 and inf on train and test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aYkjzTzuXA7"
      },
      "source": [
        "## TODO #3 Write Bigram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4s7oSmjkNuU"
      },
      "source": [
        "class BigramModel():\n",
        "  def __init__(self, data, vocab):\n",
        "    self.bigram_count = defaultdict(lambda: 0.0)\n",
        "    self.unigram_count = defaultdict(lambda: 0.0)\n",
        "    self.vocab = vocab\n",
        "    self.total_bigrams = 0\n",
        "\n",
        "    for sentence in data:\n",
        "        for w1, w2 in sentence:\n",
        "              self.bigram_count[(w1, w2)] += 1.0\n",
        "              self.unigram_count[w1] += 1.0\n",
        "              self.total_bigrams += 1\n",
        "\n",
        "  def __getitem__(self, bigram):\n",
        "    w1, w2 = bigram\n",
        "    if w1 in self.vocab and w2 in self.vocab:\n",
        "        bigram_prob = self.bigram_count[bigram] / self.unigram_count[w1]\n",
        "        return bigram_prob\n",
        "    else:\n",
        "        return 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3_Cgu6guXA-"
      },
      "source": [
        "## TODO #4 Write Perplexity for Bigram Model\n",
        "\n",
        "Sum perplexity score at a sentence level, instead of word level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hICoAhZjAxo1"
      },
      "source": [
        "def perplexity(bigram_data,model):\n",
        "    total_ln_prob = 0.0\n",
        "    total_word_count = len(bigram_data[0])\n",
        "\n",
        "    for w1, w2 in bigram_data[0]:\n",
        "        ln_prob = getLnValue(model[(w1, w2)])\n",
        "        total_ln_prob += ln_prob\n",
        "    return math.exp(-total_ln_prob / total_word_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_bigrams = [list(ngrams(sent, n=2)) for sent in tokenized_train]\n",
        "test_bigrams = [list(ngrams(sent, n=2)) for sent in tokenized_test]"
      ],
      "metadata": {
        "id": "NxJYI3_TS2gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model_scratch = BigramModel(train_bigrams, vocab)"
      ],
      "metadata": {
        "id": "A4DD_RPFtxUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(perplexity([list(flatten(train_bigrams))], bigram_model_scratch))\n",
        "print(perplexity([list(flatten(test_bigrams))[:16]], bigram_model_scratch)) #can be used to compare with nltk\n",
        "print(perplexity([list(flatten(test_bigrams))], bigram_model_scratch))"
      ],
      "metadata": {
        "id": "yw4BubpbtuV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39fffea-ae23-4e1b-b438-271c194a247f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56.45504870219316\n",
            "24.598691603876187\n",
            "inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2 MCV"
      ],
      "metadata": {
        "id": "PRv294uQcZFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_test_bigrams = [list(ngrams(sent, n=2)) for sent in tokenized_wiki_test]"
      ],
      "metadata": {
        "id": "kCeRCyOIUWTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(perplexity([list(flatten(wiki_test_bigrams))],bigram_model_scratch))"
      ],
      "metadata": {
        "id": "q47hutRqIg1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71edbc30-464e-4f1f-a1ae-6bebe7406b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BAF9DQbuXBC"
      },
      "source": [
        "# Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlm75BWLuXBC"
      },
      "source": [
        "Usually any ngram models have a sparsity problem, which means it does not have every possible ngram of words in the dataset. Smoothing techniques can alleviate this problem. In this section, you will implement three basic smoothing methods laplace smoothing, interpolation for bigram, and Knesey-Ney smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwa7YQiouXBD"
      },
      "source": [
        "## TODO #5 write Bigram with Laplace smoothing (Add-One Smoothing)\n",
        "\n",
        "The result perplexity on training and testing should be:\n",
        "\n",
        "    370.28, 369.16 for Laplace smoothing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramWithLaplaceSmoothing():\n",
        "\n",
        "  def __init__(self, data, vocab):\n",
        "    self.bigram_count = defaultdict(lambda: 0.0)\n",
        "    self.unigram_count = defaultdict(lambda: 0.0)\n",
        "    self.vocab = vocab\n",
        "    self.total_bigrams = 0\n",
        "\n",
        "    for sentence in data:\n",
        "        for w1, w2 in sentence:\n",
        "              self.bigram_count[(w1, w2)] += 1.0\n",
        "              self.unigram_count[w1] += 1.0\n",
        "              self.total_bigrams += 1\n",
        "\n",
        "  def __getitem__(self, bigram):\n",
        "    w1, w2 = bigram\n",
        "    if w1 in self.vocab and w2 in self.vocab:\n",
        "        bigram_prob = (self.bigram_count[bigram] + 1) / (self.unigram_count[w1] +len(self.vocab))\n",
        "        return bigram_prob\n",
        "    else:\n",
        "        return 1 / (self.unigram_count[w1] +len(self.vocab))\n",
        "\n",
        "model = BigramWithLaplaceSmoothing(train_bigrams, vocab)\n",
        "print(perplexity([list(flatten(train_bigrams))],model))\n",
        "print(perplexity([list(flatten(test_bigrams))], model))"
      ],
      "metadata": {
        "id": "j2Bw4C9T_UEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9660da9f-cd2e-4585-cf51-5ac9ef0460c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "370.28232024056035\n",
            "369.1605485652555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3 MCV"
      ],
      "metadata": {
        "id": "mFT4uhIGhP0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(perplexity([list(flatten(wiki_test_bigrams))],model))"
      ],
      "metadata": {
        "id": "jSH60cshIpDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2ac883-cacc-4e0d-e52b-0fc428d63e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "735.0253999215859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JDswBSIuXBG"
      },
      "source": [
        "## TODO #6 Write Bigram with Interpolation\n",
        "Set the lambda value as 0.7 for bigram, 0.25 for unigram, and 0.05 for unknown word.\n",
        "\n",
        "The result perplexity on training and testing should be:\n",
        "\n",
        "    70.07, 102.99 for Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramWithInterpolation():\n",
        "\n",
        "  def __init__(self, data, vocab):\n",
        "    self.bigram_count = defaultdict(lambda: 0.0)\n",
        "    self.unigram_count = defaultdict(lambda: 0.0)\n",
        "    self.vocab = vocab\n",
        "    self.vocab_size = len(vocab)\n",
        "\n",
        "    for sentence in data:\n",
        "        for w1, w2 in sentence:\n",
        "            self.bigram_count[(w1,w2)] += 1.0\n",
        "            self.unigram_count[w1] += 1.0\n",
        "            self.vocab_size += 1\n",
        "\n",
        "        self.unigram_count[w2] += 1.0\n",
        "        self.vocab_size += 1\n",
        "\n",
        "  def __getitem__(self, bigram):\n",
        "    w1, w2 = bigram\n",
        "    unigram_prob = self.unigram_count[w2]/self.vocab_size\n",
        "    bigram_prob = self.bigram_count[(w1,w2)]/self.unigram_count[w1]\n",
        "\n",
        "    return 0.7*bigram_prob + 0.25*unigram_prob + 0.05*(1/len(self.vocab))\n",
        "\n",
        "model = BigramWithInterpolation(train_bigrams, vocab)\n",
        "print(perplexity([list(flatten(train_bigrams))],model))\n",
        "print(perplexity([list(flatten(test_bigrams))], model))"
      ],
      "metadata": {
        "id": "PIeDBLarvZUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58d9b42-30ba-4f64-a572-f5845586c1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70.13089958685914\n",
            "103.1619568899827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4 MCV"
      ],
      "metadata": {
        "id": "i-GlmJUIhN7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(perplexity([list(flatten(wiki_test_bigrams))],model))"
      ],
      "metadata": {
        "id": "EilXywU-IuNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15cd3fa2-bda8-47ed-dd9f-e0cafb6d48ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "252.2239389680049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUorP-EWuXBM"
      },
      "source": [
        "## Language modeling on multiple domains\n",
        "\n",
        "Sometimes, we do not have enough data to create a language model for a new domain. In that case, we can improvised by combining several models to improve result on the new domain.\n",
        "\n",
        "In this exercise you will try to merge two language models from news and article domains to create a language model for the encyclopedia domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jel9Hx69uXBN"
      },
      "source": [
        "# create encyclopeida data (test data)\n",
        "encyclo_data=[]\n",
        "with open('BEST2010/encyclopedia.txt','r',encoding='utf-8') as f:\n",
        "    for i,line in enumerate(f):\n",
        "        encyclo_data.append(line.strip()[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(news) First, you should try to calculate perplexity of your bigram with interpolation on encyclopedia data. The  perplexity should be around 236.33"
      ],
      "metadata": {
        "id": "Jlla-S8YYRur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_encyclo_data = [ed.split(\"|\") for ed in encyclo_data]\n",
        "tokenized_encyclo_data = [[token if token in vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_encyclo_data]\n",
        "\n",
        "padded_tokenized_encyclo_data = [list(pad_both_ends(sentence, n=2)) for sentence in tokenized_encyclo_data]\n",
        "encyclopedia_bigrams = [list(ngrams(sent, n=2)) for sent in padded_tokenized_encyclo_data]"
      ],
      "metadata": {
        "id": "gkRm8W4UWyfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0l91qLEuXBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c93b25-0702-447d-fb14-92edefc31ed4"
      },
      "source": [
        "# 1) news only on \"encyclopedia\"\n",
        "print(perplexity([list(flatten(encyclopedia_bigrams))], model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "236.9009503507002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwV9j9U-uXBR"
      },
      "source": [
        "## TODO #7 - Langauge Modelling on Multiple Domains\n",
        "Combine news and article datasets to create another bigram model and evaluate it on the encyclopedia data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9skdgo8muXBO"
      },
      "source": [
        "\n",
        "\n",
        "(article) For your information, a bigram model with interpolation using article data to test on encyclopedia data has a perplexity of 218.57"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOA8fd53uXBU"
      },
      "source": [
        "# 2) article only on \"encyclopedia\"\n",
        "best2010_article=[]\n",
        "with open('BEST2010/article.txt','r',encoding='utf-8') as f:\n",
        "    for i,line in enumerate(f):\n",
        "        best2010_article.append(line.strip()[:-1])\n",
        "\n",
        "combined_total_word_count = 0\n",
        "for line in best2010_article:\n",
        "    combined_total_word_count += len(line.split('|'))\n",
        "\n",
        "# --- Code Here ---\n",
        "# Build the vocabulary\n",
        "tokenized_articles = [[\"<s>\"] + article.split(\"|\") + [\"</s>\"] for article in best2010_article]\n",
        "flattened_tokens = list(flatten(tokenized_articles))\n",
        "article_vocab = Vocabulary(flattened_tokens, unk_cutoff=3)\n",
        "\n",
        "# Tokenize and handle unknown tokens\n",
        "tokenized_articles = [line.split(\"|\") for line in best2010_article]\n",
        "tokenized_articles = [[token if token in article_vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_articles]\n",
        "\n",
        "# Add padding and generate bigrams\n",
        "padded_articles = [list(pad_both_ends(sentence, n=2)) for sentence in tokenized_articles]\n",
        "article_bigrams = [list(ngrams(sentence, n=2)) for sentence in padded_articles]\n",
        "# --- Code Here ---\n",
        "\n",
        "model = BigramWithInterpolation(article_bigrams, article_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Perplexity of the bigram model using article data with interpolation smoothing on encyclopedia test data',perplexity([list(flatten(encyclopedia_bigrams))], model))"
      ],
      "metadata": {
        "id": "7bLYcPvXYHkB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "a733e084-c7ce-4556-c593-4e55c93e7bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "float division by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-e7f631bcbfb0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Perplexity of the bigram model using article data with interpolation smoothing on encyclopedia test data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencyclopedia_bigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-5d0d59d2840c>\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(bigram_data, model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigram_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mln_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtotal_ln_prob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mln_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtotal_ln_prob\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_word_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-922c3500ca51>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, bigram)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0munigram_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munigram_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbigram_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigram_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munigram_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbigram_prob\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munigram_prob\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBjmLhUcuXBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290a63e0-d147-4128-e777-b036989bf9ed"
      },
      "source": [
        "# 3) train on news + article, test on \"encyclopedia\"\n",
        "best2010_article_and_news = best2010_article.copy()\n",
        "with open('BEST2010/news.txt','r',encoding='utf-8') as f:\n",
        "    for i,line in enumerate(f):\n",
        "        best2010_article_and_news.append(line.strip()[:-1])\n",
        "\n",
        "# --- Code Here ---\n",
        "# Build the vocabulary\n",
        "combined_article = best2010_article_and_news + best2010_article\n",
        "tokenized_articles = [[\"<s>\"] + article.split(\"|\") + [\"</s>\"] for article in combined_article]\n",
        "flattened_tokens = list(flatten(tokenized_articles))\n",
        "combined_vocab = Vocabulary(flattened_tokens, unk_cutoff=3)\n",
        "\n",
        "# Tokenize and handle unknown tokens\n",
        "tokenized_articles = [line.split(\"|\") for line in combined_article]\n",
        "tokenized_articles = [[token if token in combined_vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_articles]\n",
        "\n",
        "# Add padding and generate bigrams\n",
        "padded_articles = [list(pad_both_ends(sentence, n=2)) for sentence in tokenized_articles]\n",
        "combined_bigrams = [list(ngrams(sentence, n=2)) for sentence in padded_articles]\n",
        "# --- Code Here ---\n",
        "combined_model = BigramWithInterpolation(combined_bigrams, combined_vocab)\n",
        "print('Perplexity of the combined Bigram model with interpolation smoothing on encyclopedia test data',perplexity([list(flatten(encyclopedia_bigrams))], combined_model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of the combined Bigram model with interpolation smoothing on encyclopedia test data 194.9846771974706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNPEhD7WuXBV"
      },
      "source": [
        "## TODO #8 - Kneser-ney on \"News\"\n",
        "\n",
        "<!-- Reimplement equation 4.33 in SLP textbook (https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf) -->\n",
        "\n",
        "Implement Bigram Knerser-ney LM. The result perplexity should be around 65.81, 93.21 on train and test data. Be careful not to mix up vocab from the above section!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramKneserNey():\n",
        "\n",
        "  def __init__(self, data, vocab):\n",
        "    self.bigram_counts = defaultdict(lambda: 0.0)\n",
        "    self.unigram_counts = defaultdict(lambda: 0.0)\n",
        "    self.total_bigrams = 0\n",
        "    self.vocab = vocab\n",
        "\n",
        "    for sentence in data:\n",
        "        for bigram in sentence:\n",
        "            self.bigram_counts[bigram] += 1.0\n",
        "            self.unigram_counts[bigram[0]] += 1.0\n",
        "            self.total_bigrams += 1\n",
        "\n",
        "    self.unique_continuations = defaultdict(lambda: 0.0)\n",
        "    for bigram in self.bigram_counts:\n",
        "        self.unique_continuations[bigram[1]] += 1.0\n",
        "\n",
        "    self.following_counts = defaultdict(set)\n",
        "    for first_word, second_word in self.bigram_counts.keys():\n",
        "        self.following_counts[first_word].add(second_word)\n",
        "\n",
        "\n",
        "  def __getitem__(self, x):\n",
        "    d = 0.75  # Discount factor\n",
        "    w1, w2 = x\n",
        "\n",
        "    # Continuation probability\n",
        "    continuation_prob = self.unique_continuations[w2] / len(self.bigram_counts)\n",
        "\n",
        "    # Discounted probability for bigrams\n",
        "    bigram_prob = max(self.bigram_counts[x] - d, 0) / self.unigram_counts[w1]\n",
        "\n",
        "    # Backoff weight\n",
        "    backoff_weight = (d / self.unigram_counts[w1]) * len(self.following_counts[w1])\n",
        "\n",
        "    return bigram_prob + backoff_weight * continuation_prob\n",
        "\n",
        "model = BigramKneserNey(train_bigrams, vocab)\n",
        "print(perplexity([list(flatten(train_bigrams))],model))\n",
        "print(perplexity([list(flatten(train_bigrams))[:1000]],model)) #can be used to compare with nltk\n",
        "print(perplexity([list(flatten(test_bigrams))[:1000]], model)) #can be used to compare with nltk\n",
        "print(perplexity([list(flatten(test_bigrams))], model))"
      ],
      "metadata": {
        "id": "Y_8xFf7tBqpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f40bb2-b8bd-4093-c076-6412953d40d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65.80901349887752\n",
            "50.40541747450013\n",
            "90.30241418790267\n",
            "94.95975861424682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5 MCV"
      ],
      "metadata": {
        "id": "ULDScRw-g8Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(perplexity([list(flatten(wiki_test_bigrams))],model))"
      ],
      "metadata": {
        "id": "eSZ1Pb9WvfWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431aca0a-99d0-44a1-cf71-5970232b854c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "289.2035227711219\n"
          ]
        }
      ]
    }
  ]
}